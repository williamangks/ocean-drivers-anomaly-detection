#!/usr/bin/env python3
"""
Ingest Chlorophyll-a (8-day composite) via ERDDAP into BigQuery standard.chl_8day.

Dataset:
- ERDDAP griddap dataset: erdMBchla8day_LonPM180
- Variable: chlorophyll (units: mg m-3)
- Longitude range: -180..180 (LonPM180), so no 0..360 conversion is needed. :contentReference[oaicite:3]{index=3}

Time handling:
- ERDDAP 'time' is "Centered Time" for each 8-day composite. :contentReference[oaicite:4]{index=4}
- We convert to a period window:
    period_start_date = center_date - 3 days
    period_end_date   = center_date + 4 days
This convention is documented and consistent for downstream alignment.
"""

from __future__ import annotations

import argparse
import datetime as dt
import urllib.request
from urllib.parse import quote
from dataclasses import dataclass
from pathlib import Path

import pandas as pd
import xarray as xr
import yaml
from google.cloud import bigquery

DEFAULT_MIN_BYTES = 1024

RAW_REQUIRED_COLS = {"time", "lat", "lon", "chl_mg_m3"}

STANDARD_COLS = [
    "period_start_date",
    "period_end_date",
    "region_id",
    "lat",
    "lon",
    "chl_mg_m3",
    "source",
    "ingested_at",
]
STANDARD_COLS_SET = set(STANDARD_COLS)

REQUIRED_COLS = [
    "period_start_date",
    "period_end_date",
    "region_id",
    "lat",
    "lon",
    "source",
    "ingested_at",
]

ERDDAP_BASE = "https://coastwatch.pfeg.noaa.gov/erddap/griddap"
DATASET_ID = "erdMBchla8day_LonPM180"
VAR_NAME = "chlorophyll"
SOURCE_NAME = "NOAA_ERDDAP_erdMBchla8day_LonPM180"

@dataclass(frozen=True)
class BoundBox:
    lat_min: float
    lat_max: float
    lon_min: float
    lon_max: float

def load_regions(path: str) -> dict[str, BoundBox]:
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    out: dict[str, BoundBox] = {}
    for region_id, spec in cfg["regions"].items():
        bb = spec["boundbox"]
        out[region_id] = BoundBox(
            lat_min=float(bb["lat_min"]),
            lat_max=float(bb["lat_max"]),
            lon_min=float(bb["lon_min"]),
            lon_max=float(bb["lon_max"]),
        )
    return out

def month_range(year: int, month: int) -> tuple[dt.date, dt.date]:
    start = dt.date(year, month, 1)
    if month == 12:
        end = dt.date(year + 1, 1, 1) - dt.timedelta(days=1)
    else:
        end = dt.date(year, month + 1, 1) - dt.timedelta(days=1)
    return start, end


def build_erddap_nc_url(date_start: dt.date, date_end: dt.date, bb: BoundBox) -> str:
    """
    griddap constraint order:
        chlorophyll[(time)][(altitude)][(latitude)][(longitude)]
    altitude is 0.0.
    """
    t0 = f"{date_start.isoformat()}T00:00:00Z"
    t1 = f"{date_end.isoformat()}T00:00:00Z"

    lat_min, lat_max = sorted([bb.lat_min, bb.lat_max])
    lon_min, lon_max = sorted([bb.lon_min, bb.lon_max])

    constraint = (
        f"{VAR_NAME}"
        f"[({t0}):1:({t1})]"
        f"[(0.0):1:(0.0)]"
        f"[({lat_min}):1:({lat_max})]"
        f"[({lon_min}):1:({lon_max})]"
    )

    return f"{ERDDAP_BASE}/{DATASET_ID}.nc?{constraint}"


def log(msg: str) -> None:
    print(f"[chl] {msg}")


def validate_netcdf_file(path: str | Path, min_bytes: int = DEFAULT_MIN_BYTES) -> tuple[bool, str]:
    p = Path(path)
    if not p.exists():
        return False, f"path={p} missing"

    size = p.stat().st_size
    if size < min_bytes:
        return False, f"path={p} too_small size={size}B min_bytes={min_bytes}"

    with p.open("rb") as f:
        head4 = f.read(4)

    if head4.startswith(b"CDF"):
        return True, f"path={p} netcdf_classic size={size}B"
    if head4 == b"\x89HDF":
        return True, f"path={p} netcdf4_hdf5 size={size}B"

    return False, f"path={p} bad_header head4={head4!r} size={size}B"


def ensure_local_netcdf(url: str, local_nc: Path, force_download: bool, min_bytes: int) -> None:
    local_nc.parent.mkdir(parents=True, exist_ok=True)
    safe_url = quote(url, safe=":/?&=()[]%")

    ok, info = validate_netcdf_file(local_nc, min_bytes=min_bytes)
    if ok and not force_download:
        log(f"using_cached_download=true ({info})")
        return

    reason = info if not ok else "force_download=true"
    log(f"using_cached_download=false (re-downloading) reason={reason}")

    urllib.request.urlretrieve(safe_url, str(local_nc))

    ok2, info2 = validate_netcdf_file(local_nc, min_bytes=min_bytes)
    if not ok2:
        raise RuntimeError(f"Downloaded file failed validation: {info2}")
    log(f"download_complete=true ({info2})")


def validate_raw_dataframe(df: pd.DataFrame) -> None:
    cols = set(df.columns)
    missing = RAW_REQUIRED_COLS - cols
    if missing:
        raise ValueError(f"Raw dataframe missing columns: {sorted(missing)}. Got: {sorted(cols)}")


def validate_standardized_dataframe(df: pd.DataFrame) -> None:
    cols = set(df.columns)
    missing = STANDARD_COLS_SET - cols
    if missing:
        raise ValueError(f"Standardized dataframe missing columns: {sorted(missing)}. Got: {sorted(cols)}")

    if df[REQUIRED_COLS].isna().any().any():
        bad = df[df[REQUIRED_COLS].isna().any(axis=1)]
        raise ValueError(f"Nulls in REQUIRED columns:\n{bad.head(10)}")


def subset_to_long(ds: xr.Dataset, region_id: str) -> pd.DataFrame:
    da = ds[VAR_NAME]

    rename_map = {}
    if "latitude" in da.coords:
        rename_map["latitude"] = "lat"
    if "longitude" in da.coords:
        rename_map["longitude"] = "lon"
    if rename_map:
        da = da.rename(rename_map)

    raw_df = da.to_dataframe(name="chl_mg_m3").reset_index()

    validate_raw_dataframe(raw_df)

    center_date = pd.to_datetime(raw_df["time"], utc=True).dt.date
    raw_df["period_start_date"] = center_date - pd.to_timedelta(3, unit="D")
    raw_df["period_end_date"] = center_date + pd.to_timedelta(4, unit="D")

    raw_df.drop(columns=["time"], inplace=True)

    raw_df["region_id"] = region_id
    raw_df["source"] = SOURCE_NAME
    raw_df["ingested_at"] = pd.Timestamp.now(tz="UTC")

    df = raw_df[STANDARD_COLS].copy()

    # Fill/missing value is -9999999.0 for chlorophyll. :contentReference[oaicite:5]{index=5}
    df.loc[df["chl_mg_m3"] <= -9990000.0, "chl_mg_m3"] = None

    validate_standardized_dataframe(df)
    return df


def delete_existing_rows(
    project: str,
    dataset: str,
    table: str,
    region_id: str,
    date_start: dt.date,
    date_end: dt.date,
) -> None:
    client = bigquery.Client(project=project)
    table_id = f"{project}.{dataset}.{table}"

    sql = f"""
    DELETE FROM `{table_id}`
    WHERE region_id = @region_id
      AND period_start_date BETWEEN @d0 AND @d1
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("region_id", "STRING", region_id),
            bigquery.ScalarQueryParameter("d0", "DATE", date_start),
            bigquery.ScalarQueryParameter("d1", "DATE", date_end),
        ]
    )
    client.query(sql, job_config=job_config).result()


def load_to_bigquery(df: pd.DataFrame, project: str, dataset: str, table: str) -> None:
    client = bigquery.Client(project=project)
    table_id = f"{project}.{dataset}.{table}"
    job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)
    client.load_table_from_dataframe(df, table_id, job_config=job_config).result()


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Ingest chlorophyll-a 8-day composites via ERDDAP into BigQuery.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument("--regions_yaml", default="src/config/regions.yaml")
    parser.add_argument("--region_id", required=True)
    parser.add_argument("--year", type=int, required=True)
    parser.add_argument("--month", type=int, required=True)
    parser.add_argument("--bq_project", required=True)
    parser.add_argument("--bq_dataset", default="standard")
    parser.add_argument("--bq_table", default="chl_8day")
    parser.add_argument("--dry_run", action="store_true")
    parser.add_argument("--out_dir", default="data/tmp")
    parser.add_argument("--replace", action="store_true")
    parser.add_argument("--force_download", action="store_true")
    parser.add_argument("--min_bytes", type=int, default=DEFAULT_MIN_BYTES)
    args = parser.parse_args()

    if not (1 <= args.month <= 12):
        parser.error("--month must be between 1 and 12")

    regions = load_regions(args.regions_yaml)
    bb = regions[args.region_id]

    d0, d1 = month_range(args.year, args.month)

    log(
        f"start region={args.region_id} period={d0}..{d1} dry_run={args.dry_run} "
        f"replace={args.replace} force_download={args.force_download} min_bytes={args.min_bytes}"
    )

    url = build_erddap_nc_url(d0, d1, bb)
    log(f"fetch_url={url}")

    out_dir = Path(args.out_dir)
    local_nc = out_dir / f"chl_{args.region_id}_{args.year}_{args.month:02d}.nc"
    log(f"download_path={local_nc}")

    ensure_local_netcdf(url, local_nc, force_download=args.force_download, min_bytes=args.min_bytes)

    with xr.open_dataset(local_nc) as ds:
        df = subset_to_long(ds, args.region_id)

    log(f"rows_ready={len(df):,}")

    if args.dry_run:
        log("dry_run: skipped BigQuery load.")
        return

    table_id = f"{args.bq_project}.{args.bq_dataset}.{args.bq_table}"

    if args.replace:
        log(f"replace=true delete_existing table={table_id} region={args.region_id} period={d0}..{d1}")
        delete_existing_rows(args.bq_project, args.bq_dataset, args.bq_table, args.region_id, d0, d1)
    else:
        log("replace=false (append only)")

    log(f"load_bq table={table_id} rows={len(df):,}")
    load_to_bigquery(df, args.bq_project, args.bq_dataset, args.bq_table)
    log(f"done table={table_id}")

if __name__ == "__main__":
    main()
